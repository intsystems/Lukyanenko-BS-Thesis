{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from IPython.display import clear_output\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os\n",
    "import re\n",
    "from functools import partial\n",
    "from typing import List\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from transformers import BertTokenizerFast\n",
    "\n",
    "from src.manipulation_helpers.data_preparation import markup_conll, encode_tags, Markup, \\\n",
    "                                                      read_markup, create_span_targeting_data, \\\n",
    "                                                      SpanTargetingDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_FULL_MARKUP = \"data/markup_union_with_empty.json\"\n",
    "PATH_TO_MATCHED = \"data/markup_union_matched.json\"\n",
    "PATH_TO_ENG_DATA = \"data/data/protechn_corpus_eval/train\"\n",
    "MODEL_NAME = \"DeepPavlov/rubert-base-cased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "text  = open(PATH_TO_ENG_DATA + f'/article{idx}.txt', 'rb').read().decode('utf-8').replace('\\n', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Confirmed: Authorities LIED About Las Vegas Shooter’s Hotel Check-In Date – What Else Are They Hiding?Over the last few days the alternative media has spent countless hours conducting their own investigations into what actually happened during the mass shooting in Las Vegas that left 59 dead and over 500 injured.From reports of multiple shooters to officials seemingly covering up the ISIS connection, many different theories have been put forth that counter the mainstream narrative.Now, new information released by investigative reporter Laura Loomer proves that authorities have directly lied to the American people about the case at least once by claiming that supposed shooter Stephen Paddock checked into the Mandalay Bay Hotel on September 28th when valet records (with photos) prove he actually arrived three days earlier.According to Loomer, she obtained the image from a source which shows that Paddock’s car first arrived September 25th.The photo even has a handwritten note that was reportedly written by an FBI agent – proving that the FBI specifically lied to the country.The picture “proves FBI misled public about #StephenPaddock’s check in date,” Loomer Tweeted.EXCLUSIVE PICS: LV Shooter’s car; @FBI note inside @MandalayBay valet center proves FBI misled public about #StephenPaddock‘s check in date.pic.twitter.com/sotjwX3o0i — Laura Loomer (@LauraLoomer) October 6, 2017Shockingly, another Tweet by Loomer also revealed that the license plate numbers given out by police after the horrific shooting DO NOT match the actual license plate of Paddock’s vehicle.Law enforcement and @FBI misled the public about #Paddock‘s check in date and also provided public & media w/ wrong license plate number.pic.twitter.com/y9hS6GqdKI — Laura Loomer (@LauraLoomer) October 6, 2017That’s right, photographic evidence from inside the hotel parking garage has confirmed that the FBI, along with state and local police, specifically lied about key details of the shooting.This throws their entire narrative into question and makes one wonder what else is being hidden from the public?Keep in mind that at least four videos from the scene of the shooting have already been released that indicate there were multiple shooters.In fact, there are so many unanswered questions that the Drudge Report even linked to an article asking them directly.As noted above, at this point literally every piece of so-called evidence put forth by authorities and then regurgitated by the mainstream media cannot be trusted and should be considered as disinformation until proven otherwise.And if Paddock really did act alone and this evidence is some sort of mistake, there is surely video footage proving so.Notice #StephenPaddock‘s van was last parked right next to a security camera inside “Garage East” @MandalayBay.There is definitely footage.https://t.co/L1caGn6uqw — Laura Loomer (@LauraLoomer) October 6, 2017It is also important to note that Paddock’s brother Eric has conducted a series of bizarre interviews that have led to even more questions about what actually happened in Las Vegas and what Paddock’s role was in the worst mass shooting in American history.'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Stop Islamization of America.\\n\\n'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eng_text[0][191: 222]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_text = [(letter, tag) for letter, tag in zip(\n",
    "    eng_text[0], \n",
    "    [0 for _ in eng_text[0]][: 191] + [1 for _ in eng_text[0]][: 31] + [0 for _ in eng_text[0]][222:]\n",
    ")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_split(tagged_text):\n",
    "    text = []\n",
    "    tags = []\n",
    "    word = ''\n",
    "    prev_tag = 0\n",
    "    for letter, tag in tagged_text:\n",
    "        if letter == ' ' or letter == '\\n':\n",
    "            if word == '':\n",
    "                continue\n",
    "                \n",
    "            text.append(word)\n",
    "            tags.append(prev_tag)\n",
    "            word = ''\n",
    "            prev_tag = 0\n",
    "        else:\n",
    "            word += letter\n",
    "            prev_tag = tag\n",
    "    return text, tags\n",
    "\n",
    "def create_bin_span(text, bounds):\n",
    "    span = [0 for _ in range(bounds[0][0])]\n",
    "    for i, bound in enumerate(bounds):\n",
    "        if i == len(bounds) - 1:\n",
    "            span += [1 for _ in range(bound[1] - bound[0])]\n",
    "            span += [0 for _ in range(len(text) - bound[1])]\n",
    "        else:\n",
    "            span += [1 for _ in range(bound[1] - bound[0])]\n",
    "            span += [0 for _ in range(bounds[i + 1][0] - bound[1])]\n",
    "    return [(letter, tag) for letter, tag in zip(text, span)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt_2 = create_bin_span(eng_text[0], [(191, 222)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tt_2 == tagged_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "text, tags = custom_split(tagged_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Stop', 'Islamization', 'of', 'America.', 'They'], [1, 1, 1, 1, 0])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[30:35], tags[30:35]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_text = []\n",
    "eng_spans = []\n",
    "\n",
    "for filename in os.listdir(PATH_TO_ENG_DATA):\n",
    "    if filename.endswith('.tsv'):\n",
    "        idx = re.search('\\d+', filename).group(0)\n",
    "        text = open(PATH_TO_ENG_DATA + f'/article{idx}.txt', 'rb').read().decode('utf-8')\n",
    "        try:\n",
    "            spans = pd.read_table(PATH_TO_ENG_DATA + f'/{filename}', header=None)\n",
    "        except:\n",
    "            continue\n",
    "        bounds = []\n",
    "        for i in range(len(spans)):\n",
    "            bounds.append((spans[2].iloc[i], spans[3].iloc[i]+1))\n",
    "        tagged_text = create_bin_span(text, bounds)\n",
    "        text, tags = custom_split(tagged_text)\n",
    "        eng_text.append(text)\n",
    "        eng_spans.append(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(eng_spans)):\n",
    "    if len(eng_text[i]) != len(eng_spans[i]):\n",
    "        print(\"PIZDA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_proc(span):\n",
    "    for i in range(len(span) - 1):\n",
    "        if span[i] == 0 and span[i + 1] == 1:\n",
    "            span[i + 1] = 2\n",
    "    return span "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_spans = list(map(post_proc, eng_spans))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eng_spans[0][30:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Всего разметок: 5868\n",
      "Всего сматченных разметок: 1686\n"
     ]
    }
   ],
   "source": [
    "markup = pd.read_json(PATH_TO_MATCHED, lines=True)\n",
    "markup_union = pd.read_json(PATH_TO_FULL_MARKUP, lines=True)\n",
    "markup_union = markup_union.merge(markup_union.groupby(['input_document_id'])['assignment_worker_id'].agg('count').reset_index(), on=['input_document_id'])\n",
    "\n",
    "\n",
    "print(\"Всего разметок:\", len(markup_union))\n",
    "print(\"Всего сматченных разметок:\", len(markup))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "markup_union = markup_union[markup_union['assignment_worker_id_y'] == 3]\n",
    "map_idx2markup = {\n",
    "    i: {'data': markup_union[markup_union['markup_index'] == i]} for i in range(3)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1488"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(map_idx2markup[1]['data'].input_document_id.unique()).intersection(set(markup.input_document_id.unique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_texts = []\n",
    "full_markups = []\n",
    "for i in range(3):\n",
    "    texts_union = []\n",
    "    markups_union = []\n",
    "    for index, row in map_idx2markup[i]['data'].iterrows():\n",
    "        text, row_markup = markup_conll(row[\"input_input\"], \n",
    "                                        row[\"output_result\"], \n",
    "                                        row[\"input_entitiesdata\"])\n",
    "\n",
    "        texts_union.append(text)\n",
    "        markups_union.append(row_markup)\n",
    "    full_texts += texts_union\n",
    "    full_markups += markups_union\n",
    "    map_idx2markup[i]['texts'] = texts_union\n",
    "    map_idx2markup[i]['markups'] = markups_union\n",
    "    clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_tags = {x.manipulation_class for m in markups_union for x in m}\n",
    "tag2id = {tag: idx for idx, tag in enumerate(unique_tags)}\n",
    "id2tag = {idx: tag for tag, idx in tag2id.items()}\n",
    "\n",
    "bio2id = {\n",
    "    'O': 0,\n",
    "    'I': 1,\n",
    "    'B': 2,\n",
    "}\n",
    "id2bio = {\n",
    "    idx : tag for tag, idx in bio2id.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained(MODEL_NAME)\n",
    "\n",
    "encode = partial(\n",
    "    tokenizer,\n",
    "    add_special_tokens=False, \n",
    "    is_split_into_words=True, \n",
    "    padding=True, \n",
    "    truncation=True, \n",
    "    max_length=512)\n",
    "\n",
    "encoded_sep_token = encode([tokenizer.sep_token])['input_ids'][0]\n",
    "encoded_cls_token = encode([tokenizer.cls_token])['input_ids'][0]\n",
    "\n",
    "for idx in map_idx2markup:\n",
    "    map_idx2markup[idx]['encodings'] = encode(map_idx2markup[idx]['texts'])\n",
    "    map_idx2markup[idx]['spans'] = encode_tags(map_idx2markup[idx]['markups'], map_idx2markup[idx]['encodings'], \"bio_span\", tag2id=bio2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Иван\\AppData\\Local\\Temp\\ipykernel_756\\3504671554.py:5: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  recall = np.nan_to_num((((span1 > 0) & (span2 > 0)).sum(-1) / (span1 > 0).sum(-1)).sum() / (span1 > 0).any().sum())\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.0, 0.0, 0.0)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def span_matrics(span1: List, span2: List):\n",
    "    span1, span2 = np.array(span1), np.array(span2)\n",
    "    if (span1 > 0).sum() == 0 and (span2 > 0).sum() == 0:\n",
    "        return None\n",
    "    recall = np.nan_to_num((((span1 > 0) & (span2 > 0)).sum(-1) / (span1 > 0).sum(-1)).sum() / (span1 > 0).any().sum())\n",
    "    precision = np.nan_to_num((((span1 > 0) & (span2 > 0)).sum(-1) / (span2 > 0).sum(-1)).sum() / (span2 > 0).any().sum())\n",
    "    cohen_kappa = cohen_kappa_score(span1[span1 != -100], span2[span1 != -100])\n",
    "    return recall, precision, cohen_kappa\n",
    "\n",
    "span_matrics(map_idx2markup[0]['spans'][0], map_idx2markup[1]['spans'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.22477731183850802\n",
      "0.22335330633831046\n",
      "0.12036711069922158\n"
     ]
    }
   ],
   "source": [
    "precisions = []\n",
    "recalls = []\n",
    "cohen_kappas = []\n",
    "\n",
    "for span1, span2 in zip(map_idx2markup[1]['spans'], map_idx2markup[0]['spans']):\n",
    "    output = span_matrics(span1, span2)\n",
    "    if output is None:\n",
    "        continue\n",
    "    p, r, ck = output\n",
    "    precisions.append(p)\n",
    "    recalls.append(r)\n",
    "    cohen_kappas.append(ck)\n",
    "    \n",
    "print(sum(precisions) / len(precisions))\n",
    "print(sum(recalls) / len(recalls))\n",
    "print(sum(cohen_kappas) / len(cohen_kappas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
